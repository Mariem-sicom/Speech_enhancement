{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import shutil\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import random\n",
    "from shutil import copyfile\n",
    "import soundfile as sf\n",
    "from scipy import signal\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import librosa.display\n",
    "import scipy.io.wavfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Data\n",
    "#Train_Directroy = '.\\Train'\n",
    "try:\n",
    "    \n",
    "    os.makedirs('/tmp/wav_files')\n",
    "    #os.makedirs('/tmp/signals')\n",
    "    os.makedirs('/tmp/wav_files/signals')\n",
    "    os.makedirs('/tmp/wav_files/specs')\n",
    "    os.makedirs('/tmp/noise/noise')\n",
    "    os.makedirs('/tmp/noise/specs')\n",
    "    os.makedirs('/tmp/noisy_signals')\n",
    "    os.makedirs('/tmp/noisy_signals/signals')\n",
    "    os.makedirs('/tmp/noisy_signals/specs')\n",
    "    os.makedirs('/tmp/noisy_signals/masks')\n",
    "    os.makedirs('/tmp/training/input')\n",
    "    os.makedirs('/tmp/training/output')\n",
    "    os.makedirs('/tmp/validation/input')\n",
    "    os.makedirs('/tmp/validation/output')\n",
    "    os.makedirs('/tmp/testing/input')\n",
    "    os.makedirs('/tmp/testing/output')\n",
    "    os.makedirs('/tmp/test_signal/noisy_signal')\n",
    "    os.makedirs('/tmp/test_signal/spec')\n",
    "    os.makedirs('/tmp/test_signal/predicted_signal')\n",
    "    os.makedirs('/tmp/source')\n",
    "    \n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "SOURCE_DIR = \"/tmp/source/\"\n",
    "TRAINING_DIR = \"/tmp/training/\"\n",
    "TESTING_DIR = \"/tmp/testing/\"\n",
    "WAV_FILES = \"/tmp/wav_files/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load the wav files\n",
    "SOURCE = \"/tmp/source/\"   # source of raw data\n",
    "DEST = \"/tmp/wav_files/signals/\"\n",
    "def load_wav_data(SOURCE, DEST):\n",
    "    list = (os.listdir(SOURCE))\n",
    "    for i in range(len(list)):\n",
    "        if(os.path.isdir(SOURCE+list[i])):\n",
    "            if (os.path.getsize(SOURCE+list[i])>0):\n",
    "                load_wav_data(SOURCE+list[i]+\"/\",DEST)     \n",
    "        elif (list[i].endswith('.WAV')):\n",
    "            #print(SOURCE+list[i])\n",
    "            shutil.copy2(SOURCE+list[i],DEST)\n",
    "            s, fs = sf.read(SOURCE+list[i])\n",
    "load_wav_data(SOURCE, DEST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE = \"/tmp/wav_files/signals/\"\n",
    "def get_min_signal(SOURCE):\n",
    "    minLenSignal = 100000;\n",
    "    list = (os.listdir(SOURCE))\n",
    "    for i in range(len(list)):    \n",
    "        if (list[i].endswith('.WAV')):\n",
    "            s, fs = sf.read(SOURCE+list[i])\n",
    "            if(len(s)<minLenSignal):\n",
    "                minLenSignal = len(s)\n",
    "    return minLenSignal,fs\n",
    "minLenSignal,fs = get_min_signal(SOURCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000\n"
     ]
    }
   ],
   "source": [
    "print(fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize signals to the size of the smallest one\n",
    "def resize_signals(SOURCE,DEST):\n",
    "    list = (os.listdir(SOURCE))\n",
    "    for i in range(len(list)):    \n",
    "        if (list[i].endswith('.WAV')):\n",
    "            s, fs = sf.read(SOURCE+list[i])\n",
    "            for j in range(int(len(s)/minLenSignal)):\n",
    "                x = s[j*minLenSignal:(j+1)*minLenSignal]\n",
    "                scipy.io.wavfile.write(DEST+'signal'+str(i+1)+'_'+str(j+1)+'.WAV',fs,x)\n",
    "#resize_signals(SOURCE,DEST)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample noise signal     \n",
    "def noise_resample(SOURCE_NOISE,fs):\n",
    "    b, fb = sf.read(SOURCE_NOISE)\n",
    "    b = signal.resample(b,int(len(b)*fs/fb))\n",
    "    return b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding noise to signals\n",
    "import math\n",
    "SOURCE_SIG = '/tmp/wav_files/signals/'\n",
    "SOURCE_NOISE = '/tmp/Bruit/babble.WAV'\n",
    "DEST_NOISY_SIGNAL = '/tmp/noisy_signals/signals/'\n",
    "DEST_NOISE = '/tmp/noise/noise/'\n",
    "\n",
    "def create_noisy_signal(SOURCE,SOURCE_NOISE,DEST_NOISY_SIGNAL,DEST_NOISE,fs):  \n",
    "    br = noise_resample(SOURCE_NOISE,fs)\n",
    "    list = (os.listdir(SOURCE))\n",
    "    list_rsb = []\n",
    "    for i in range(len(list)):\n",
    "        print(list[i])\n",
    "        if (list[i].endswith('.WAV')):\n",
    "            s, fs = sf.read(SOURCE+list[i])\n",
    "            num = random.randint(0,(len(br)-len(s))) \n",
    "            Ps=sum(s*s)\n",
    "            Pbr=sum(br[num:(num+len(s))]*br[num:(num+len(s))])\n",
    "            #alpha= math.sqrt(Ps/(Pbr)) #rsb=0\n",
    "            #alpha= math.sqrt(Ps/(Pbr*0.25)) #rsb=-6db\n",
    "            alpha= math.sqrt(Ps/(Pbr*3.98)) #rsb=+6db\n",
    "            b=alpha*br[num:(num+len(s))]\n",
    "            Pb=sum(b*b)\n",
    "            Nsignal = s+b   #Noisy signal\n",
    "            rsb=10*math.log10(Ps/(Pb))\n",
    "            list_rsb.append(rsb)\n",
    "            scipy.io.wavfile.write(DEST_NOISY_SIGNAL+list[i],fs,Nsignal)\n",
    "            scipy.io.wavfile.write(DEST_NOISE+list[i],fs,b)\n",
    "    return list_rsb\n",
    "\n",
    "list_rsb=create_noisy_signal(SOURCE_SIG,SOURCE_NOISE,DEST_NOISY_SIGNAL,DEST_NOISE,fs)\n",
    "#print(list_rsb)\n",
    "plt.plot(list_rsb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert signal to spectrogram \n",
    "def signal_to_spec(SOURCE, DEST):\n",
    "    list = (os.listdir(SOURCE))\n",
    "    for i in range(len(list)):\n",
    "        if (os.path.getsize(SOURCE+list[i])>0 & list[i].endswith('.WAV')):\n",
    "            s, fs = sf.read(SOURCE+list[i])\n",
    "            f_s, t_s, Zss = signal.stft(s, fs, nperseg=512, window='hann', noverlap=256)\n",
    "            print(Zss.shape)\n",
    "            #Sdb = librosa.amplitude_to_db(abs(Zss))\n",
    "            #Sdb_normalized= (Sdb-np.min(Sdb))/(np.max(Sdb)-np.min(Sdb))\n",
    "            #plt.pcolormesh(t_s, f_s,Sdb_normalized,  shading='auto')\n",
    "            #librosa.display.specshow(Sdb, sr=fs, x_axis='time', y_axis='hz')\n",
    "            #plt.title('STFT Magnitude of Sdb')\n",
    "            #plt.ylabel('Frequency [Hz]')\n",
    "            #plt.xlabel('Time [sec]')\n",
    "            with open(DEST+'spec'+list[i][0:-4]+'.npy', 'wb') as f:\n",
    "                np.save(f, np.array(Zss))\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create specs of noise\n",
    "SOURCE = '/tmp/noise/noise/';\n",
    "DEST = '/tmp/noise/specs/';            \n",
    "signal_to_spec(SOURCE, DEST)\n",
    "\n",
    "# create specs of signals\n",
    "SOURCE = '/tmp/wav_files/signals/'\n",
    "DEST = '/tmp/wav_files/specs/'\n",
    "signal_to_spec(SOURCE, DEST)\n",
    "\n",
    "# create specs of noisy signals\n",
    "SOURCE = '/tmp/noisy_signals/signals/'\n",
    "DEST = '/tmp/noisy_signals/specs/'\n",
    "signal_to_spec(SOURCE, DEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_SIGNAL_SPECS = '/tmp/wav_files/specs/'\n",
    "SOURCE_NOISE_SPECS = '/tmp/noise/specs/'\n",
    "DEST_MASK = '/tmp/noisy_signals/masks/'\n",
    "\n",
    "def signal_to_mask(SOURCE_SIGNAL_SPECS,SOURCE_NOISE_SPECS, DEST_MASK):\n",
    "    list_sig = (os.listdir(SOURCE_SIGNAL_SPECS))\n",
    "    list_noise = (os.listdir(SOURCE_NOISE_SPECS))\n",
    "    for i in range(len(list_sig)):\n",
    "        with open(SOURCE_SIGNAL_SPECS+list_sig[i], 'rb') as f: \n",
    "            #SIG = librosa.amplitude_to_db(abs(np.load(f))) \n",
    "            SIG = abs(np.load(f))\n",
    "        with open(SOURCE_NOISE_SPECS+list_noise[i], 'rb') as f:\n",
    "            #NOISE = librosa.amplitude_to_db(abs(np.load(f)))\n",
    "            NOISE = abs(np.load(f))\n",
    "        RSB = (SIG**2)/(NOISE**2)     # parce que amplitude en dB\n",
    "        where_are_NaNs = np.isnan(RSB)\n",
    "        RSB[where_are_NaNs] = 1\n",
    "        RSB[RSB>=0.9]=1\n",
    "        RSB[RSB<0.9]=0\n",
    "        Mask = RSB\n",
    "        \"\"\"\n",
    "        print(Mask)\n",
    "        with open('/tmp/noisy_signals/specs/'+'specSX416.npy', 'rb') as f:\n",
    "            s = np.load(f)\n",
    "        print(s.shape)\n",
    "        t_new,s_new = signal.istft(s*Mask, fs, nperseg=1024, window='hann',nfft= 4096, noverlap=256);\n",
    "        print(s_new.shape)\n",
    "        scipy.io.wavfile.write('/tmp'+'/masked_signal.wav',fs,s_new)\n",
    "        print(\"eee\")\n",
    "        \"\"\"\n",
    "        with open(DEST_MASK+'mask'+list_sig[i][4:-4]+'.npy', 'wb') as f:\n",
    "            np.save(f, np.array(Mask))\n",
    "signal_to_mask(SOURCE_SIGNAL_SPECS,SOURCE_NOISE_SPECS, DEST_MASK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_SPEC = '/tmp/noisy_signals/specs/'\n",
    "SOURCE_MASK = '/tmp/noisy_signals/masks/'\n",
    "TRAINING = '/tmp/training/'\n",
    "TESTING = '/tmp/testing/'\n",
    "VALIDATION = '/tmp/validation/'\n",
    "#Split the training set and the test set.\n",
    "def split_data(SOURCE_SPEC, SOURCE_MASK, TRAINING, TESTING, train_ratio,val_ratio):\n",
    "    #list_sig = random.sample(os.listdir(SOURCE), len(os.listdir(SOURCE)))\n",
    "    list_specs = os.listdir(SOURCE_SPEC)\n",
    "    list_mask = os.listdir(SOURCE_MASK)\n",
    "    #shuffle data\n",
    "    ziped_lists = list(zip(list_specs, list_mask))\n",
    "    random.shuffle(ziped_lists)\n",
    "    list_specs, list_mask = zip(*ziped_lists)\n",
    "    \n",
    "    for i in range(len(list_specs)):\n",
    "        print(os.path.getsize(SOURCE_MASK+list_mask[i]))\n",
    "        if(os.path.getsize(SOURCE_SPEC+list_specs[i])>0 and os.path.getsize(SOURCE_MASK+list_mask[i])>0):\n",
    "            if (i < int(train_ratio*len(list_specs))):\n",
    "                shutil.copy2(SOURCE_SPEC+list_specs[i],TRAINING+'input/')\n",
    "                shutil.copy2(SOURCE_MASK+list_mask[i],TRAINING+'output/')\n",
    "            elif (i < int((train_ratio+val_ratio)*len(list_specs))):\n",
    "                shutil.copy2(SOURCE_SPEC+list_specs[i],VALIDATION+'input/')\n",
    "                shutil.copy2(SOURCE_MASK+list_mask[i],VALIDATION+'output/')\n",
    "            else:\n",
    "                shutil.copy2(SOURCE_SPEC+list_specs[i],TESTING+'input/')\n",
    "                shutil.copy2(SOURCE_MASK+list_mask[i],TESTING+'output/')\n",
    "                \n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "split_data(SOURCE_SPEC, SOURCE_MASK, TRAINING, TESTING, train_ratio, val_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlapping_indexes(len_array,size,hop):\n",
    "    #size = 4\n",
    "    #overlap = 1\n",
    "    A = list(range(len_array))\n",
    "    # length = length of the array that we want to split\n",
    "    # size = size of each resulting chunk\n",
    "    # hop = number of non overlaping samples between two chunks\n",
    "    indexes = list(zip(*[A[i:] for i in range(size)]))[::hop]\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and reshape data\n",
    "def load_reshape_data(SOURCE_DATA):\n",
    "    \n",
    "    list_data = os.listdir(SOURCE_DATA+'input/')\n",
    "    list_labels = os.listdir(SOURCE_DATA+'output/')\n",
    "\n",
    "    Data = []\n",
    "    Labels = []\n",
    "    \n",
    "    chunk_size = 7\n",
    "    hop = 1\n",
    "    \n",
    "    for i in range(len(list_data)):\n",
    "        #print(list_data[i])\n",
    "        with open(SOURCE_DATA+'input/'+list_data[i], 'rb') as f:\n",
    "            Sdb = librosa.amplitude_to_db(abs(np.load(f)))\n",
    "            Sdb_normalized= (Sdb-np.min(Sdb))/(np.max(Sdb)-np.min(Sdb))\n",
    "            indexes = overlapping_indexes(Sdb_normalized.shape[1],chunk_size,hop)\n",
    "            kmin = int(chunk_size/2)\n",
    "            kmax = kmin+len(indexes)\n",
    "            for k in range(kmax-kmin):\n",
    "                Data.append(Sdb_normalized[:,indexes[k]])\n",
    "\n",
    "        with open(SOURCE_DATA+'output/'+list_labels[i], 'rb') as f:\n",
    "            mask = np.load(f)\n",
    "            split_mask = np.split(mask[:,kmin:kmax],(kmax-kmin), axis=1)\n",
    "            Labels = Labels+split_mask\n",
    "            \n",
    "    data_array = np.array(Data)\n",
    "    labels_array = np.array(Labels)\n",
    "    \n",
    "    data_array = np.transpose(data_array, (0,2,1)) \n",
    "\n",
    "    data_array = np.expand_dims(data_array, axis = 3)  \n",
    "            \n",
    "    return data_array,labels_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(169904, 7, 257, 1) (169904, 257, 1)\n"
     ]
    }
   ],
   "source": [
    "SOURCE_TRAIN = '/tmp/training/'\n",
    "input_train_data,output_train_data = load_reshape_data(SOURCE_TRAIN)\n",
    "print(input_train_data.shape,output_train_data.shape )\n",
    "with open(SOURCE_TRAIN+'input_train1_data1.npy', 'wb') as f:\n",
    "    np.save(f, np.array(input_train_data))\n",
    "with open(SOURCE_TRAIN+'output_train1_data1.npy', 'wb') as f:\n",
    "    np.save(f, np.array(output_train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(input_train_data[10,:,0:10])\n",
    "#print(output_train_data[10,0:10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22023, 7, 257, 1) (22023, 257, 1)\n"
     ]
    }
   ],
   "source": [
    "SOURCE_TEST = '/tmp/testing/'\n",
    "input_test_data,output_test_data = load_reshape_data(SOURCE_TEST) \n",
    "print(input_test_data.shape,output_test_data.shape )\n",
    "with open(SOURCE_TEST+'input_test1_data1.npy', 'wb') as f:\n",
    "    np.save(f, np.array(input_test_data))\n",
    "with open(SOURCE_TEST+'output_test1_data1.npy', 'wb') as f:\n",
    "    np.save(f, np.array(output_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20693, 7, 257, 1) (20693, 257, 1)\n"
     ]
    }
   ],
   "source": [
    "SOURCE_VAL = '/tmp/validation/'\n",
    "input_val_data,output_val_data = load_reshape_data(SOURCE_VAL) \n",
    "print(input_val_data.shape,output_val_data.shape )\n",
    "with open(SOURCE_VAL+'input_val1_data1.npy', 'wb') as f:\n",
    "    np.save(f, np.array(input_val_data))\n",
    "with open(SOURCE_VAL+'output_val1_data1.npy', 'wb') as f:\n",
    "    np.save(f, np.array(output_val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get('binary_accuracy')>= 0.95):\n",
    "            print(\"\\nReached 95% accuracy so cancelling training!\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    \n",
    "    tf.keras.layers.Conv2D(16, (3,3),padding = 'same', activation='relu',input_shape=(7, 257, 1)),\n",
    "    tf.keras.layers.Conv2D(32, (3,3), padding = 'same',activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(1,3),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), padding = 'same',activation='relu'), \n",
    "    #tf.keras.layers.Conv2D(32, (3,3), padding = 'same',activation='relu'), \n",
    "    tf.keras.layers.MaxPooling2D(1,3),\n",
    "    #tf.keras.layers.MaxPooling2D(1,12),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(512, activation='relu'), \n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1024, activation='relu'), \n",
    "    tf.keras.layers.Dense(257, activation='sigmoid'), \n",
    "    ])\n",
    "\n",
    "# Compile Model. \n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])\n",
    "callbacks = myCallback()\n",
    "# Train the Model\n",
    "history = model.fit(input_train_data, output_train_data, epochs=30 , verbose = 1,callbacks=[callbacks],validation_data = (input_val_data,output_val_data))\n",
    "\n",
    "#batch_size =10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model')\n",
    "#model = tf.keras.models.load_model('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT LOSS AND ACCURACY\n",
    "%matplotlib inline\n",
    "import matplotlib.image  as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#-----------------------------------------------------------\n",
    "# sets for each training epoch\n",
    "#-----------------------------------------------------------\n",
    "acc=history.history['binary_accuracy']\n",
    "val_acc=history.history['val_binary_accuracy']\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "\n",
    "epochs=range(len(acc)) # Get number of epochs\n",
    "\n",
    "#------------------------------------------------\n",
    "# Plot training and validation accuracy per epoch\n",
    "#------------------------------------------------\n",
    "plt.plot(epochs, acc, 'r')#, \"Training Accuracy\")\n",
    "plt.plot(epochs, val_acc, 'b')#, \"Validation Accuracy\")\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.figure()\n",
    "\n",
    "\n",
    "#------------------------------------------------\n",
    "# Plot training and validation loss per epoch\n",
    "#------------------------------------------------\n",
    "plt.plot(epochs, loss, 'r')#, \"Training Loss\")\n",
    "plt.plot(epochs, val_loss, 'b')#, \"Validation Loss\")\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.figure()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(input_test_data, output_test_data, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donnee = input_train_data[0,:,:,:]\n",
    "donnee = np.expand_dims(donnee, axis = 0)\n",
    "donnee.shape\n",
    "Mask = model.predict(donnee)\n",
    "print(Mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = output_train_data[0,:]\n",
    "sum(sum(abs(Mask-labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = tf.keras.metrics.BinaryAccuracy()\n",
    "m.update_state(labels, Mask)\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the spectrogram of the test signal and reshape it\n",
    "def get_input(SOURCE,chunk_size,hop):\n",
    "    input_signal = os.listdir(SOURCE)\n",
    "    Data=[]\n",
    "    with open(SOURCE+input_signal[0], 'rb') as f:\n",
    "        SComplex = np.load(f)                                # complex spectrogram\n",
    "        Sdb = librosa.amplitude_to_db(abs(SComplex))\n",
    "        Sdb_normalized= (Sdb-np.min(Sdb))/(np.max(Sdb)-np.min(Sdb))\n",
    "        indexes = overlapping_indexes(Sdb_normalized.shape[1],chunk_size,hop)\n",
    "        kmin = int(chunk_size/2)\n",
    "        kmax = kmin+len(indexes)\n",
    "        for k in range(kmax-kmin):\n",
    "            #print(B[:,indexes[k]])\n",
    "            #print(Sdb_normalized[:,indexes[k]].shape)\n",
    "            Data.append(Sdb_normalized[:,indexes[k]])\n",
    "        input_data = np.array(Data)\n",
    "        SComplex = SComplex[:,kmin:kmax]\n",
    "    input_data = np.transpose(input_data, (0,2,1)) \n",
    "    input_data = np.expand_dims(input_data, axis = 3)\n",
    "    print(input_data.shape)\n",
    "    print(SComplex.shape)\n",
    "    return SComplex,input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE = \"/tmp/test_signal/noisy_signal/\"\n",
    "DEST_SPEC = \"/tmp/test_signal/spec/\"\n",
    "DEST_SIGNAL = \"/tmp/test_signal/predicted_signal\"\n",
    "chunk_size = 7\n",
    "hop = 1\n",
    "def denoise_signal(SOURCE,DEST_SPEC,DEST_SIGNAL,chunk_size,hop):\n",
    "    # create the spectrogram of the test signal\n",
    "    signal_to_spec(SOURCE, DEST_SPEC)\n",
    "    spec, input_data = get_input(DEST_SPEC,chunk_size,hop)\n",
    "    denoised_spec = np.zeros((spec.shape[0],spec.shape[1]),dtype=complex)\n",
    "    for i in range(input_data.shape[0]):\n",
    "        spec_chunk = np.expand_dims(input_data[i,:,:,:], axis = 0)\n",
    "        denoised_chunk = model.predict(spec_chunk)\n",
    "        #denoised_chunk = denoised_chunk.reshape((chunk_size,-1)).T\n",
    "        denoised_chunk[denoised_chunk>=0.5] = 1\n",
    "        denoised_chunk[denoised_chunk<0.5] = 0\n",
    "        denoised_spec[:,i] = spec[:,i]*denoised_chunk #input_data[i,:,:,0].T\n",
    "    print(denoised_spec.shape)\n",
    "    print(input_data[i,:,:,0].T.shape)\n",
    "    t_new,s_new=signal.istft(denoised_spec, fs, nperseg=512, window='hann',noverlap=256);\n",
    "    scipy.io.wavfile.write(DEST_SIGNAL+'/signal.wav',fs,s_new)\n",
    "    return s_new\n",
    "x = denoise_signal(SOURCE,DEST_SPEC,DEST_SIGNAL,chunk_size,hop)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
